{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dff9af-cee9-45c7-980d-5dcfe21cdd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "file_1990_1999 = pd.read_csv('D:/GUVI_Projects/My_Projects/singapur sheets/ResaleFlatPricesBasedonApprovalDate19901999.csv')\n",
    "file_2000_2012 = pd.read_csv('D:/GUVI_Projects/My_Projects/singapur sheets/ResaleFlatPricesBasedonApprovalDate2000Feb2012.csv')\n",
    "file_2012_2014 = pd.read_csv('D:/GUVI_Projects/My_Projects/singapur sheets/ResaleFlatPricesBasedonRegistrationDateFromMar2012toDec2014.csv')\n",
    "file_2015_2016 = pd.read_csv('D:/GUVI_Projects/My_Projects/singapur sheets/ResaleFlatPricesBasedonRegistrationDateFromJan2015toDec2016.csv')\n",
    "file_2017_onwards = pd.read_csv('D:/GUVI_Projects/My_Projects/singapur sheets/ResaleflatpricesbasedonregistrationdatefromJan2017onwards.csv')\n",
    "\n",
    "# Standardize column names across datasets\n",
    "file_1990_1999.columns = file_1990_1999.columns.str.lower().str.replace(' ', '_')\n",
    "file_2000_2012.columns = file_2000_2012.columns.str.lower().str.replace(' ', '_')\n",
    "file_2012_2014.columns = file_2012_2014.columns.str.lower().str.replace(' ', '_')\n",
    "file_2015_2016.columns = file_2015_2016.columns.str.lower().str.replace(' ', '_')\n",
    "file_2017_onwards.columns = file_2017_onwards.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Find common columns across all datasets\n",
    "common_columns = list(set(file_1990_1999.columns) & set(file_2000_2012.columns) & set(file_2012_2014.columns) & set(file_2015_2016.columns) & set(file_2017_onwards.columns))\n",
    "\n",
    "# Select only common columns in each DataFrame\n",
    "file_1990_1999 = file_1990_1999[common_columns]\n",
    "file_2000_2012 = file_2000_2012[common_columns]\n",
    "file_2012_2014 = file_2012_2014[common_columns]\n",
    "file_2015_2016 = file_2015_2016[common_columns]\n",
    "file_2017_onwards = file_2017_onwards[common_columns]\n",
    "\n",
    "# Concatenate all datasets\n",
    "all_files = pd.concat([file_1990_1999, file_2000_2012, file_2012_2014, file_2015_2016, file_2017_onwards], ignore_index=True)\n",
    "\n",
    "# Display the combined dataset\n",
    "print(all_files.head())\n",
    "\n",
    "# Save the combined dataset to a CSV file\n",
    "all_files.to_csv('D:/Final_Projects/project_new/Project1_resale_flat_prices.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e4200f-4b7e-486f-ba95-b0d70a5e6899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename=\"error_log.log\", level=logging.DEBUG,\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(use_label_encoder=False, eval_metric='rmse')\n",
    "}\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "dataset_path = 'D:/Final_Projects/project_new/Project1_resale_flat_prices.csv'\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Step 2: Initial Exploration of the Dataset\n",
    "print(\"Initial Dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Step 3: Inspect the Dataset\n",
    "print(\"\\nDataset Info:\")\n",
    "data.info()\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(data.describe())\n",
    "print(\"\\nColumn Data Types:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# Step 4: Check for Missing Values\n",
    "print(\"\\nMissing Values in Each Column:\")\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values[missing_values > 0])  # Only show columns with missing values\n",
    "\n",
    "# Step 5: Handle Missing Values\n",
    "data = data.dropna()  # Consider using data.fillna() for more control\n",
    "\n",
    "# Display the dataset info again to ensure there are no missing values\n",
    "print(\"\\nDataset Info After Handling Missing Values:\")\n",
    "data.info()\n",
    "\n",
    "# Step 6: Data Type Conversions\n",
    "data['month'] = pd.to_datetime(data['month'])\n",
    "\n",
    "# Step 7: Feature Extraction and Engineering\n",
    "data['year'] = data['month'].dt.year\n",
    "data['month'] = data['month'].dt.month\n",
    "\n",
    "# Display the first few rows to verify the new features\n",
    "print(\"\\nDataset After Feature Engineering:\")\n",
    "print(data.head())\n",
    "\n",
    "# Step 8: Standardize unique values in 'flat_model' and 'flat_type' columns\n",
    "# Define mappings to handle variants (e.g., \"multi-generation\" vs \"multi generation\")\n",
    "flat_model_map = {\n",
    "    'multi-generation': 'multi_generation', \n",
    "    'multi generation': 'multi_generation'\n",
    "}\n",
    "\n",
    "flat_type_map = {\n",
    "    '5 room': '5_room',\n",
    "    '5-room': '5_room',\n",
    "    '3 room': '3_room',\n",
    "    '3-room': '3_room',\n",
    "    'executive apartment': 'executive_apartment',\n",
    "    'executive maisonette': 'executive_maisonette',\n",
    "}\n",
    "\n",
    "# Apply standardization\n",
    "data['flat_model'] = data['flat_model'].str.lower().replace(flat_model_map)\n",
    "data['flat_type'] = data['flat_type'].str.lower().replace(flat_type_map)\n",
    "\n",
    "# Step 9: Verify the unique values after standardization\n",
    "print(\"\\nUnique flat_model values after standardization:\", data['flat_model'].unique())\n",
    "print(\"Unique flat_type values after standardization:\", data['flat_type'].unique())\n",
    "\n",
    "# Step 10: Exploratory Data Analysis (EDA)\n",
    "# Step A: Visualize distributions of numerical features\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data['resale_price'], kde=True, bins=50)\n",
    "plt.title('Distribution of Resale Prices')\n",
    "plt.xlabel('Resale Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data['floor_area_sqm'], kde=True, bins=50)\n",
    "plt.title('Distribution of Floor Area (sqm)')\n",
    "plt.xlabel('Floor Area (sqm)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Step B: Visualize correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "numeric_data = data.select_dtypes(include=[np.number])\n",
    "correlation_matrix = numeric_data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Step C: Visualize relationships between features and target\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='floor_area_sqm', y='resale_price', data=data)\n",
    "plt.title('Resale Price vs. Floor Area (sqm)')\n",
    "plt.xlabel('Floor Area (sqm)')\n",
    "plt.ylabel('Resale Price')\n",
    "plt.show()\n",
    "\n",
    "# Step 11: Save the Cleaned Data\n",
    "cleaned_dataset_path = 'D:/Final_Projects/project_new/latest_cleaned_singapore_resale_flat_prices.csv'\n",
    "data.to_csv(cleaned_dataset_path, index=False)\n",
    "print(f\"\\nCleaned dataset saved at: {cleaned_dataset_path}\")\n",
    "\n",
    "# Step 12: Prepare the Features and Target Variable\n",
    "data = pd.read_csv(cleaned_dataset_path)\n",
    "\n",
    "X = data[['floor_area_sqm', 'storey_range', 'lease_commence_date', 'year', 'month', 'flat_model', 'town', 'flat_type', 'street_name']]\n",
    "y = data['resale_price']\n",
    "\n",
    "# Step 13: Encode Categorical Variables\n",
    "X = pd.get_dummies(X, columns=['storey_range', 'flat_model', 'town', 'flat_type', 'street_name'], drop_first=True)\n",
    "\n",
    "# Display the first few rows of the encoded features\n",
    "print(\"\\nEncoded Features (X):\")\n",
    "print(X.head())\n",
    "\n",
    "# Step 14: Split the Dataset into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the training and testing sets\n",
    "print(\"\\nShapes of Training and Testing Sets:\")\n",
    "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}, y_train: {y_train.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Step 15: Define and Train Machine Learning Models\n",
    "results = {}  # Initialize results dictionary\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    try:\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = mse ** 0.5  # Calculate RMSE from MSE\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        logging.debug(f\"{model_name} - MAE: {mae:.2f}, MSE: {mse:.2f}, RMSE: {rmse:.2f}, R2: {r2:.2f}\")\n",
    "        print(f\"\\n{model_name} - MAE: {mae:.2f}, MSE: {mse:.2f}, RMSE: {rmse:.2f}, R2: {r2:.2f}\")\n",
    "\n",
    "        # Store results\n",
    "        results[model_name] = {\n",
    "            'MAE': mae,\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'R2': r2,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error training {model_name}: {str(e)}\")\n",
    "\n",
    "# Step 16: Cross-Validation for the Best Model\n",
    "\n",
    "if results:\n",
    "    best_model_name = max(results, key=lambda x: results[x]['R2'])  # Choose the model with the highest R2\n",
    "\n",
    "    try:\n",
    "        best_model = models[best_model_name]\n",
    "\n",
    "        # Perform k-fold cross-validation on the best model\n",
    "        kfold = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "        cv_scores = []\n",
    "\n",
    "        for train_index, val_index in kfold.split(X):\n",
    "            X_train_cv, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "            y_train_cv, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "            best_model.fit(X_train_cv, y_train_cv)\n",
    "            y_pred_cv = best_model.predict(X_val)\n",
    "            score = r2_score(y_val, y_pred_cv)\n",
    "            cv_scores.append(score)\n",
    "\n",
    "        logging.debug(f\"Cross-Validation R2 Scores for {best_model_name}: {cv_scores}\")\n",
    "        logging.debug(f\"Mean Cross-Validation R2 Score: {np.mean(cv_scores):.2f}\")\n",
    "        \n",
    "        print(f\"\\nCross-Validation R2 Scores for {best_model_name}: {cv_scores}\")\n",
    "        print(f\"Mean Cross-Validation R2 Score: {np.mean(cv_scores):.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error performing cross-validation for {best_model_name}: {str(e)}\")\n",
    "else:\n",
    "    print(\"No results available to determine the best model.\")\n",
    "\n",
    "# Step 17: Save the Best Model\n",
    "\n",
    "if 'best_model_name' in locals():  # Check if best_model_name is defined\n",
    "    model_path = f'D:/Final_Projects/project_new/{best_model_name.replace(\" \", \"_\").lower()}_model.joblib'\n",
    "\n",
    "    try:\n",
    "        joblib.dump(best_model, model_path)\n",
    "        logging.debug(f\"Best Model saved at: {model_path}\")\n",
    "        print(f\"\\nBest Model saved at: {model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving best model: {str(e)}\")\n",
    "else:\n",
    "    print(\"No best model to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c040d38-d01a-4bd5-a72c-efe90cc1761c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique flat_model values after standardization: ['improved' 'new generation' 'model a' 'standard' 'simplified'\n",
      " 'model a-maisonette' 'apartment' 'maisonette' 'terrace' '2-room'\n",
      " 'improved-maisonette' 'multi_generation' 'premium apartment'\n",
      " 'adjoined flat' 'premium maisonette' 'model a2' 'dbss' 'type s1'\n",
      " 'premium apartment loft' '3gen']\n",
      "Unique flat_type values after standardization: ['1 room' '3_room' '4 room' '5_room' '2 room' 'executive'\n",
      " 'multi generation' 'multi-generation']\n",
      "\n",
      "Cleaned dataset saved at: D:/Final_Projects/project_new/latest_cleaned_singapore_resale_flat_prices.csv\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "\n",
      "Random Forest - MAE: 20096.53, MSE: 862841410.73, RMSE: 29374.16, R2: 0.96\n",
      "\n",
      "Best Model saved at: D:/Final_Projects/project_new/random_forest_best_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename=\"error_log.log\", level=logging.DEBUG,\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "dataset_path = 'D:/Final_Projects/project_new/Project1_resale_flat_prices.csv'\n",
    "data = pd.read_csv(dataset_path, low_memory=False)  # Optimize memory usage by setting low_memory\n",
    "\n",
    "# Step 2: Remove Outliers in Resale Price (using IQR method)\n",
    "Q1 = data['resale_price'].quantile(0.25)\n",
    "Q3 = data['resale_price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outlier_condition = ~((data['resale_price'] < (Q1 - 1.5 * IQR)) | \n",
    "                      (data['resale_price'] > (Q3 + 1.5 * IQR)))\n",
    "data = data.loc[outlier_condition]\n",
    "\n",
    "# Step 3: Handle Missing Values (drop them for simplicity in this case)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Step 4: Feature Engineering\n",
    "data['month'] = pd.to_datetime(data['month'])\n",
    "data['year'] = data['month'].dt.year\n",
    "data['month'] = data['month'].dt.month\n",
    "\n",
    "# Step 5: Standardize Categorical Variables (in-place for space efficiency)\n",
    "flat_model_map = {\n",
    "    'multi-generation': 'multi_generation',\n",
    "    'multi generation': 'multi_generation'\n",
    "}\n",
    "\n",
    "flat_type_map = {\n",
    "    '5 room': '5_room',\n",
    "    '5-room': '5_room',\n",
    "    '3 room': '3_room',\n",
    "    '3-room': '3_room',\n",
    "}\n",
    "\n",
    "data['flat_model'] = data['flat_model'].str.lower()\n",
    "data['flat_model'].replace(flat_model_map, inplace=True)\n",
    "\n",
    "data['flat_type'] = data['flat_type'].str.lower()\n",
    "data['flat_type'].replace(flat_type_map, inplace=True)\n",
    "\n",
    "# Step 6: Verify the unique values after standardization\n",
    "print(\"\\nUnique flat_model values after standardization:\", data['flat_model'].unique())\n",
    "print(\"Unique flat_type values after standardization:\", data['flat_type'].unique())\n",
    "\n",
    "# Step 7: Save the Cleaned Data\n",
    "cleaned_dataset_path = 'D:/Final_Projects/project_new/latest_cleaned_singapore_resale_flat_prices.csv'\n",
    "data.to_csv(cleaned_dataset_path, index=False)\n",
    "print(f\"\\nCleaned dataset saved at: {cleaned_dataset_path}\")\n",
    "\n",
    "# Step 8: Prepare the Features and Target Variable\n",
    "X = data[['floor_area_sqm', 'storey_range', 'lease_commence_date', 'year', 'month', 'flat_model', 'town', 'flat_type']]\n",
    "y = data['resale_price']\n",
    "\n",
    "# Step 9: Encode Categorical Variables using One-Hot Encoding (minimizing memory usage)\n",
    "X = pd.get_dummies(X, columns=['storey_range', 'flat_model', 'town', 'flat_type'], drop_first=True, dtype=np.uint8)\n",
    "\n",
    "# Step 10: Split the Dataset into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 11: Define the RandomForestRegressor Model\n",
    "model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Step 12: Define Hyperparameter Grid (reduce size to optimize time)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],  # Reduce the number of estimators\n",
    "    'max_depth': [10, 20],       # Focus on a smaller range of depths\n",
    "    'min_samples_split': [2, 5],  # Use fewer splits to reduce search time\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Step 13: Use GridSearchCV for Hyperparameter Tuning (Optimized for Time and Memory)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',\n",
    "    cv=3,  # Reduce cross-validation folds to save time\n",
    "    n_jobs=-1,  # Utilize all cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the grid search model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Step 14: Make Predictions on the Test Set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Step 15: Evaluate the Model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "logging.debug(f\"Random Forest - MAE: {mae:.2f}, MSE: {mse:.2f}, RMSE: {rmse:.2f}, R2: {r2:.2f}\")\n",
    "print(f\"\\nRandom Forest - MAE: {mae:.2f}, MSE: {mse:.2f}, RMSE: {rmse:.2f}, R2: {r2:.2f}\")\n",
    "\n",
    "# Step 16: Save the Best Model\n",
    "model_path = 'D:/Final_Projects/project_new/random_forest_best_model.joblib'\n",
    "joblib.dump(best_model, model_path, compress=3)  # Compress the model to save space\n",
    "logging.debug(f\"Best Model saved at: {model_path}\")\n",
    "print(f\"\\nBest Model saved at: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292a767c-88be-477d-b4e2-fbe09bfecb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9422a0f-70ac-49e2-a6f3-dd572dc18b86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
